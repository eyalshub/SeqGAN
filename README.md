# SeqGAN
Emotion-Driven Text Generation using SeqGAN :A full implementation of a text generation system using SeqGAN tailored for emotional expression. Combines supervised and reinforcement learning, custom reward functions (BLEU, emotion accuracy, diversity), and advanced sampling techniques (Top-k, Top-p, n-gram &amp; POS penalties).


# ğŸ­ Emotion-Driven Text Generation with SeqGAN

This repository contains the full implementation of a custom **SeqGAN-based system** for generating emotionally expressive text.
The project blends **supervised learning**, **reinforcement learning**, and **advanced sampling techniques** to teach a neural network how to write with emotional depth.

---

## ğŸ§  Project Overview

After building a classifier to identify emotions in text, this project takes the next step â€” generating sentences that reflect specific emotions (e.g., **fear**, **pride**, **embarrassment**).

### Model Architecture:
- **Generator**: LSTM with Bahdanau Attention.
- **Discriminator**: BiLSTM + CNN with dual-head outputs (Real/Fake + Emotion Classification).
- **Training Loop**: Pretraining (MLE), then adversarial training with Reinforcement Learning (RL).

### Reinforcement Learning:
Reward signal is a weighted combination of:
- **BLEU Score** â€“ n-gram overlap with reference texts.
- **Emotion Accuracy** â€“ using a pre-trained emotion classifier.
- **Discriminator Score** â€“ coherence and realism.
- **Perplexity** â€“ language fluency.
- **Repetition Penalty** â€“ avoids loops and redundancy.
- **Saliency Score** â€“ emphasizes meaningful content.


---

## ğŸ§ª Sampling Strategy

The generator uses a robust sampling system including:
- **Temperature Sampling**
- **Top-k / Top-p Filtering**
- **n-gram Penalties (Jaccard + repetition)**
- **POS-based Penalties** (e.g., repeated verbs)
- **Content Constraints** â€“ avoids functional word loops and ensures informative tokens

> These manipulations increase diversity, coherence, and emotional alignment.


---

## ğŸ“Š Visualization & Analysis

All training stages are logged and visualized:
- Generator & Discriminator pretraining
- RL loss and reward dynamics
- Distinct-n diversity metrics
- Emotion-specific output quality by temperature

ğŸ‘‰ A full analysis of the results is included in the accompanying PDF.


---

## ğŸ”— Try It Out

You can explore the results and generated texts by running the final sampling function, specifying the desired emotion. Example:
```python
samples = generate_from_generator(generator, word_to_index, index_to_word, num_samples=5, temperature=0.9, mode="temperature")
```

---

## ğŸ“ Files
- `generator.py` â€“ Model + training logic
- `discriminator.py` â€“ Dual-head classifier
- `sampling.py` â€“ Smart sampling logic
- `rewards.py` â€“ Reward function components
- `train.py` â€“ Full training loop
- `visuals/` â€“ Contains graphs and analysis

---

## ğŸ¤ Contributing
Pull requests and suggestions are welcome!
If you find the project useful, give it a â­ and share your thoughts.

---

## ğŸ“« Contact
Feel free to reach out via LinkedIn if you want to collaborate or just geek out over GANs and emotions.

---

## ğŸ§  Inspiration
This project was inspired by the challenge of not only understanding emotions â€” but teaching a machine to **express** them.
